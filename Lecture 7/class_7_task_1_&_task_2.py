# -*- coding: utf-8 -*-
"""Class 7-Task 1 & Task 2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aeQ_nx7huldpQPqZJEDltHJv_BMn0AT2
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# Define a function for the linear equation y = mx + c
def linear_function(x, m, c):
    return m * x + c

# Generate 100 evenly spaced values between -10 and 10
x_values = np.linspace(-10, 10, 100)

# Set slope (m) and intercept (c)
m = 2
c = 3

# Compute corresponding y values
y_values = linear_function(x_values, m, c)

# Plot the linear function
plt.plot(x_values, y_values, label=f'Linear Function y = {m}x + {c}', color='purple')

# Label the axes
plt.xlabel('x')
plt.ylabel('y')

# Display a legend
plt.legend()

# Enable grid
plt.grid(True)

# Show the plot
plt.show()

def mse(y_actual, y_predicted):
 return np.mean((y_actual - y_predicted) ** 2)

y_actual= np.array([1,2,3,4,5])
y_predicted= np.array([1.1,2.9,3.3,4.2,5.3])
print("Mean Squared Erorr :" ,mse(y_actual, y_predicted))

import numpy as np

def gradient_descent(X, y, learning_rate=0.01, iterations=1000):
    # Initialize slope (m) and intercept (c) to 0
    m, c = 0, 0
    n = len(y)  # Number of observations

    # Loop through the number of iterations
    for _ in range(iterations):
        # Calculate predicted values using the current m and c
        y_pred = m * X + c

        # Compute gradients
        dm = (-2/n) * sum(X * (y - y_pred))  # Gradient w.r.t m
        dc = (-2/n) * sum(y - y_pred)  # Gradient w.r.t c

        # Update parameters using the learning rate
        m -= learning_rate * dm
        c -= learning_rate * dc

    # Return the optimized values of m and c
    return m, c

# Define X as a NumPy array of input values
X = np.array([1, 2, 3, 4, 5])

# Define y as a NumPy array of actual values
y = np.array([2, 4, 6, 8, 10])

# Call the gradient_descent function to compute optimized m and c
m, c = gradient_descent(X, y)

# Print the optimized values of m and c
print("Optimized Slope (m):", m)
print("Optimized Intercept (c):", c)

#Task01 : House Price Prediction System

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

df = pd.read_csv("house_data.csv")

non_numeric_columns = df.select_dtypes(include=["object"]).columns.tolist()
print(f"Non-Numeric Columns: {non_numeric_columns}")

if non_numeric_columns:
    df = pd.get_dummies(df, columns=non_numeric_columns, drop_first=True)

if "price" not in df.columns:
   raise KeyError("The dataset does not contain a 'price' column. Please check the CSV file.")

X =  df.drop(columns=["price"])
y = df["price"]

if not np.issubdtype(X.dtypes.values[0], np.number):
  raise ValueError("Some features are still non-numeric. Check the dataset preprocessing.")

model = LinearRegression()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Evaluate model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Model Evaluation:\nMSE: {mse:.2f}, R-squared: {r2:.2f}')

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

#2. Salary Prediction System (web app for this is executed in pycharm)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

# Load dataset (Replace with actual dataset)
df = pd.read_csv('salary_data.csv')

# Preprocess data: Remove missing values
df.dropna(inplace=True)

# Identify categorical columns
categorical_columns = ['degree', 'job_role', 'location']
existing_categorical_columns = [col for col in categorical_columns if col in df.columns]

# Apply one-hot encoding only if columns exist
if existing_categorical_columns:
    df = pd.get_dummies(df, columns=existing_categorical_columns, drop_first=True)

# Define features and target variable
if 'Salary' in df.columns:
    X = df.drop(columns=['Salary'])
    y = df['Salary']

    # Ensure all feature columns are numeric
    if not np.all([np.issubdtype(dtype, np.number) for dtype in X.dtypes]):
        raise ValueError("Some features are still non-numeric. Check data preprocessing.")

    # Split dataset into training and testing
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Train Linear Regression model
    model = LinearRegression()
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Evaluate model
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f'MSE: {mse:.2f}')
    print(f'R-squared: {r2:.2f}')

    # Example prediction
    sample_input = X_test.iloc[[0]]  # Keep feature names
    predicted_salary = model.predict(sample_input)
    print(f'Predicted Salary for sample input: {predicted_salary[0]:.2f}')

    # Visualization: Actual vs Predicted Salaries
    plt.figure(figsize=(8, 6))
    sns.scatterplot(x=y_test, y=y_pred, alpha=0.7)
    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='dashed')  # 45-degree line
    plt.xlabel("Actual Salary")
    plt.ylabel("Predicted Salary")
    plt.title("Actual vs Predicted Salaries")
    plt.show()

else:
    print("Error: The 'Salary' column is missing from the dataset.")

